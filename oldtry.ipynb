{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0f8969d-bfb2-4cf3-9dd0-b37ad4c37b04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/300] Train Loss=1.054726, Train R^2=-0.0004\n",
      "[Epoch 2/300] Train Loss=0.999412, Train R^2=-0.0002\n",
      "[Epoch 3/300] Train Loss=1.000072, Train R^2=-0.0001\n",
      "[Epoch 4/300] Train Loss=0.999426, Train R^2=-0.0000\n",
      "[Epoch 5/300] Train Loss=0.999456, Train R^2=-0.0000\n",
      "[Epoch 6/300] Train Loss=0.999902, Train R^2=-0.0000\n",
      "[Epoch 7/300] Train Loss=1.001040, Train R^2=-0.0000\n",
      "[Epoch 8/300] Train Loss=1.000236, Train R^2=-0.0000\n",
      "[Epoch 9/300] Train Loss=1.000502, Train R^2=-0.0000\n",
      "[Epoch 10/300] Train Loss=1.000273, Train R^2=-0.0000\n",
      "[Epoch 11/300] Train Loss=1.000115, Train R^2=-0.0000\n",
      "[Epoch 12/300] Train Loss=0.999089, Train R^2=-0.0000\n",
      "[Epoch 13/300] Train Loss=1.001155, Train R^2=-0.0000\n",
      "[Epoch 14/300] Train Loss=1.000389, Train R^2=-0.0000\n",
      "[Epoch 15/300] Train Loss=1.000396, Train R^2=-0.0000\n",
      "[Epoch 16/300] Train Loss=1.000389, Train R^2=-0.0000\n",
      "[Epoch 17/300] Train Loss=1.000117, Train R^2=-0.0000\n",
      "[Epoch 18/300] Train Loss=0.999266, Train R^2=-0.0000\n",
      "[Epoch 19/300] Train Loss=0.999763, Train R^2=-0.0000\n",
      "[Epoch 20/300] Train Loss=0.999754, Train R^2=-0.0000\n",
      "[Epoch 21/300] Train Loss=1.000014, Train R^2=-0.0000\n",
      "[Epoch 22/300] Train Loss=1.000164, Train R^2=-0.0000\n",
      "[Epoch 23/300] Train Loss=1.000818, Train R^2=-0.0000\n",
      "[Epoch 24/300] Train Loss=0.999440, Train R^2=-0.0000\n",
      "[Epoch 25/300] Train Loss=0.999960, Train R^2=-0.0000\n",
      "[Epoch 26/300] Train Loss=0.999962, Train R^2=-0.0000\n",
      "[Epoch 27/300] Train Loss=1.000356, Train R^2=-0.0000\n",
      "[Epoch 28/300] Train Loss=0.999067, Train R^2=-0.0000\n",
      "[Epoch 29/300] Train Loss=1.000284, Train R^2=-0.0000\n",
      "[Epoch 30/300] Train Loss=0.999823, Train R^2=-0.0000\n",
      "[Epoch 31/300] Train Loss=0.998817, Train R^2=-0.0000\n",
      "[Epoch 32/300] Train Loss=0.999649, Train R^2=-0.0000\n",
      "[Epoch 33/300] Train Loss=1.000673, Train R^2=-0.0000\n",
      "[Epoch 34/300] Train Loss=0.999521, Train R^2=-0.0000\n",
      "[Epoch 35/300] Train Loss=1.000113, Train R^2=-0.0000\n",
      "[Epoch 36/300] Train Loss=1.000337, Train R^2=-0.0000\n",
      "[Epoch 37/300] Train Loss=1.000468, Train R^2=-0.0000\n",
      "[Epoch 38/300] Train Loss=0.999886, Train R^2=-0.0000\n",
      "[Epoch 39/300] Train Loss=0.999377, Train R^2=-0.0000\n",
      "[Epoch 40/300] Train Loss=0.999779, Train R^2=-0.0000\n",
      "[Epoch 41/300] Train Loss=0.999894, Train R^2=-0.0000\n",
      "[Epoch 42/300] Train Loss=0.999976, Train R^2=-0.0000\n",
      "[Epoch 43/300] Train Loss=1.000150, Train R^2=-0.0000\n",
      "[Epoch 44/300] Train Loss=0.999020, Train R^2=-0.0000\n",
      "[Epoch 45/300] Train Loss=0.999717, Train R^2=-0.0000\n",
      "[Epoch 46/300] Train Loss=1.000115, Train R^2=-0.0000\n",
      "[Epoch 47/300] Train Loss=0.998856, Train R^2=-0.0000\n",
      "[Epoch 48/300] Train Loss=0.999627, Train R^2=-0.0000\n",
      "[Epoch 49/300] Train Loss=0.998948, Train R^2=-0.0000\n",
      "[Epoch 50/300] Train Loss=0.999947, Train R^2=-0.0000\n",
      "[Epoch 51/300] Train Loss=0.999563, Train R^2=-0.0000\n",
      "[Epoch 52/300] Train Loss=0.999809, Train R^2=-0.0000\n",
      "[Epoch 53/300] Train Loss=1.000216, Train R^2=-0.0000\n",
      "[Epoch 54/300] Train Loss=0.999678, Train R^2=-0.0000\n",
      "[Epoch 55/300] Train Loss=1.000731, Train R^2=-0.0000\n",
      "[Epoch 56/300] Train Loss=1.000994, Train R^2=-0.0000\n",
      "[Epoch 57/300] Train Loss=0.999570, Train R^2=-0.0000\n",
      "[Epoch 58/300] Train Loss=0.999636, Train R^2=-0.0000\n",
      "[Epoch 59/300] Train Loss=0.999375, Train R^2=-0.0000\n",
      "[Epoch 60/300] Train Loss=0.999810, Train R^2=-0.0000\n",
      "[Epoch 61/300] Train Loss=1.000103, Train R^2=-0.0000\n",
      "[Epoch 62/300] Train Loss=0.999774, Train R^2=-0.0000\n",
      "[Epoch 63/300] Train Loss=1.000649, Train R^2=-0.0000\n",
      "[Epoch 64/300] Train Loss=1.000388, Train R^2=-0.0000\n",
      "[Epoch 65/300] Train Loss=0.999543, Train R^2=-0.0000\n",
      "[Epoch 66/300] Train Loss=0.999832, Train R^2=-0.0000\n",
      "[Epoch 67/300] Train Loss=0.999633, Train R^2=-0.0000\n",
      "[Epoch 68/300] Train Loss=1.000065, Train R^2=-0.0000\n",
      "[Epoch 69/300] Train Loss=0.999251, Train R^2=-0.0000\n",
      "[Epoch 70/300] Train Loss=1.000566, Train R^2=-0.0000\n",
      "[Epoch 71/300] Train Loss=0.999587, Train R^2=-0.0000\n",
      "[Epoch 72/300] Train Loss=0.998893, Train R^2=-0.0000\n",
      "[Epoch 73/300] Train Loss=0.999190, Train R^2=-0.0000\n",
      "[Epoch 74/300] Train Loss=1.000401, Train R^2=-0.0000\n",
      "[Epoch 75/300] Train Loss=0.999774, Train R^2=-0.0000\n",
      "[Epoch 76/300] Train Loss=0.999835, Train R^2=-0.0000\n",
      "[Epoch 77/300] Train Loss=0.999836, Train R^2=-0.0000\n",
      "[Epoch 78/300] Train Loss=0.999603, Train R^2=-0.0000\n",
      "[Epoch 79/300] Train Loss=0.999421, Train R^2=-0.0000\n",
      "[Epoch 80/300] Train Loss=1.000401, Train R^2=-0.0000\n",
      "[Epoch 81/300] Train Loss=0.999920, Train R^2=-0.0000\n",
      "[Epoch 82/300] Train Loss=0.999809, Train R^2=-0.0000\n",
      "[Epoch 83/300] Train Loss=1.000459, Train R^2=-0.0000\n",
      "[Epoch 84/300] Train Loss=0.999713, Train R^2=-0.0000\n",
      "[Epoch 85/300] Train Loss=0.999125, Train R^2=-0.0000\n",
      "[Epoch 86/300] Train Loss=1.000807, Train R^2=-0.0000\n",
      "[Epoch 87/300] Train Loss=0.999825, Train R^2=-0.0000\n",
      "[Epoch 88/300] Train Loss=1.000432, Train R^2=-0.0000\n",
      "[Epoch 89/300] Train Loss=0.999860, Train R^2=-0.0000\n",
      "[Epoch 90/300] Train Loss=0.999673, Train R^2=-0.0000\n",
      "[Epoch 91/300] Train Loss=1.000051, Train R^2=-0.0000\n",
      "[Epoch 92/300] Train Loss=1.000216, Train R^2=-0.0000\n",
      "[Epoch 93/300] Train Loss=1.000290, Train R^2=-0.0000\n",
      "[Epoch 94/300] Train Loss=0.999772, Train R^2=-0.0000\n",
      "[Epoch 95/300] Train Loss=1.000054, Train R^2=-0.0000\n",
      "[Epoch 96/300] Train Loss=0.999595, Train R^2=-0.0000\n",
      "[Epoch 97/300] Train Loss=1.000636, Train R^2=-0.0000\n",
      "[Epoch 98/300] Train Loss=1.000399, Train R^2=-0.0000\n",
      "[Epoch 99/300] Train Loss=1.000441, Train R^2=-0.0000\n",
      "[Epoch 100/300] Train Loss=0.999328, Train R^2=-0.0000\n",
      "[Epoch 101/300] Train Loss=0.999667, Train R^2=-0.0000\n",
      "[Epoch 102/300] Train Loss=0.999452, Train R^2=-0.0000\n",
      "[Epoch 103/300] Train Loss=1.000204, Train R^2=-0.0000\n",
      "[Epoch 104/300] Train Loss=0.999384, Train R^2=-0.0000\n",
      "[Epoch 105/300] Train Loss=1.000280, Train R^2=-0.0000\n",
      "[Epoch 106/300] Train Loss=1.000409, Train R^2=-0.0000\n",
      "[Epoch 107/300] Train Loss=0.999774, Train R^2=-0.0000\n",
      "[Epoch 108/300] Train Loss=1.000315, Train R^2=-0.0000\n",
      "[Epoch 109/300] Train Loss=1.000263, Train R^2=-0.0000\n",
      "[Epoch 110/300] Train Loss=1.000154, Train R^2=-0.0000\n",
      "[Epoch 111/300] Train Loss=1.000098, Train R^2=-0.0000\n",
      "[Epoch 112/300] Train Loss=1.000209, Train R^2=-0.0000\n",
      "[Epoch 113/300] Train Loss=0.999583, Train R^2=-0.0000\n",
      "[Epoch 114/300] Train Loss=1.000267, Train R^2=-0.0000\n",
      "[Epoch 115/300] Train Loss=1.000751, Train R^2=-0.0000\n",
      "[Epoch 116/300] Train Loss=0.999952, Train R^2=-0.0000\n",
      "[Epoch 117/300] Train Loss=0.999250, Train R^2=-0.0000\n",
      "[Epoch 118/300] Train Loss=0.999686, Train R^2=-0.0000\n",
      "[Epoch 119/300] Train Loss=0.999943, Train R^2=-0.0000\n",
      "[Epoch 120/300] Train Loss=1.000516, Train R^2=-0.0000\n",
      "[Epoch 121/300] Train Loss=1.001241, Train R^2=-0.0000\n",
      "[Epoch 122/300] Train Loss=0.999460, Train R^2=-0.0000\n",
      "[Epoch 123/300] Train Loss=0.999207, Train R^2=-0.0000\n",
      "[Epoch 124/300] Train Loss=1.000230, Train R^2=-0.0000\n",
      "[Epoch 125/300] Train Loss=1.000561, Train R^2=-0.0000\n",
      "[Epoch 126/300] Train Loss=1.000022, Train R^2=-0.0000\n",
      "[Epoch 127/300] Train Loss=1.000866, Train R^2=-0.0000\n",
      "[Epoch 128/300] Train Loss=0.999986, Train R^2=-0.0000\n",
      "[Epoch 129/300] Train Loss=0.999682, Train R^2=-0.0000\n",
      "[Epoch 130/300] Train Loss=1.000742, Train R^2=-0.0000\n",
      "[Epoch 131/300] Train Loss=1.000535, Train R^2=-0.0000\n",
      "[Epoch 132/300] Train Loss=1.000307, Train R^2=-0.0000\n",
      "[Epoch 133/300] Train Loss=1.001052, Train R^2=-0.0000\n",
      "[Epoch 134/300] Train Loss=1.000319, Train R^2=-0.0000\n",
      "[Epoch 135/300] Train Loss=1.000320, Train R^2=-0.0000\n",
      "[Epoch 136/300] Train Loss=0.999391, Train R^2=-0.0000\n",
      "[Epoch 137/300] Train Loss=0.999584, Train R^2=-0.0000\n",
      "[Epoch 138/300] Train Loss=0.999785, Train R^2=-0.0000\n",
      "[Epoch 139/300] Train Loss=0.999957, Train R^2=-0.0000\n",
      "[Epoch 140/300] Train Loss=0.999638, Train R^2=-0.0000\n",
      "[Epoch 141/300] Train Loss=0.999565, Train R^2=-0.0000\n",
      "[Epoch 142/300] Train Loss=0.999206, Train R^2=-0.0000\n",
      "[Epoch 143/300] Train Loss=1.000230, Train R^2=-0.0000\n",
      "[Epoch 144/300] Train Loss=1.001234, Train R^2=-0.0000\n",
      "[Epoch 145/300] Train Loss=1.000475, Train R^2=-0.0000\n",
      "[Epoch 146/300] Train Loss=0.999095, Train R^2=-0.0000\n",
      "[Epoch 147/300] Train Loss=0.999524, Train R^2=-0.0000\n",
      "[Epoch 148/300] Train Loss=1.000331, Train R^2=-0.0000\n",
      "[Epoch 149/300] Train Loss=0.999813, Train R^2=-0.0000\n",
      "[Epoch 150/300] Train Loss=0.999002, Train R^2=-0.0000\n",
      "[Epoch 151/300] Train Loss=0.998926, Train R^2=-0.0000\n",
      "[Epoch 152/300] Train Loss=1.000206, Train R^2=-0.0000\n",
      "[Epoch 153/300] Train Loss=1.000513, Train R^2=-0.0000\n",
      "[Epoch 154/300] Train Loss=0.999017, Train R^2=-0.0000\n",
      "[Epoch 155/300] Train Loss=1.000578, Train R^2=-0.0000\n",
      "[Epoch 156/300] Train Loss=1.000355, Train R^2=-0.0000\n",
      "[Epoch 157/300] Train Loss=1.000013, Train R^2=-0.0000\n",
      "[Epoch 158/300] Train Loss=1.002129, Train R^2=-0.0000\n",
      "[Epoch 159/300] Train Loss=0.999959, Train R^2=-0.0000\n",
      "[Epoch 160/300] Train Loss=1.000022, Train R^2=-0.0000\n",
      "[Epoch 161/300] Train Loss=0.999819, Train R^2=-0.0000\n",
      "[Epoch 162/300] Train Loss=1.000122, Train R^2=-0.0000\n",
      "[Epoch 163/300] Train Loss=0.999388, Train R^2=-0.0000\n",
      "[Epoch 164/300] Train Loss=0.999954, Train R^2=-0.0000\n",
      "[Epoch 165/300] Train Loss=1.000187, Train R^2=-0.0000\n",
      "[Epoch 166/300] Train Loss=0.999751, Train R^2=-0.0000\n",
      "[Epoch 167/300] Train Loss=0.999834, Train R^2=-0.0000\n",
      "[Epoch 168/300] Train Loss=1.000880, Train R^2=-0.0000\n",
      "[Epoch 169/300] Train Loss=0.999532, Train R^2=-0.0000\n",
      "[Epoch 170/300] Train Loss=1.000745, Train R^2=-0.0000\n",
      "[Epoch 171/300] Train Loss=1.000363, Train R^2=-0.0000\n",
      "[Epoch 172/300] Train Loss=0.999050, Train R^2=-0.0000\n",
      "[Epoch 173/300] Train Loss=0.999340, Train R^2=-0.0000\n",
      "[Epoch 174/300] Train Loss=0.999685, Train R^2=-0.0000\n",
      "[Epoch 175/300] Train Loss=0.999919, Train R^2=-0.0000\n",
      "[Epoch 176/300] Train Loss=0.999351, Train R^2=-0.0000\n",
      "[Epoch 177/300] Train Loss=1.000245, Train R^2=-0.0000\n",
      "[Epoch 178/300] Train Loss=0.999939, Train R^2=-0.0000\n",
      "[Epoch 179/300] Train Loss=0.999967, Train R^2=-0.0000\n",
      "[Epoch 180/300] Train Loss=1.000442, Train R^2=-0.0000\n",
      "[Epoch 181/300] Train Loss=1.000390, Train R^2=-0.0000\n",
      "[Epoch 182/300] Train Loss=1.000877, Train R^2=-0.0000\n",
      "[Epoch 183/300] Train Loss=1.000377, Train R^2=-0.0000\n",
      "[Epoch 184/300] Train Loss=0.998478, Train R^2=-0.0000\n",
      "[Epoch 185/300] Train Loss=0.999599, Train R^2=-0.0000\n",
      "[Epoch 186/300] Train Loss=0.999917, Train R^2=-0.0000\n",
      "[Epoch 187/300] Train Loss=1.000238, Train R^2=-0.0000\n",
      "[Epoch 188/300] Train Loss=0.999818, Train R^2=-0.0000\n",
      "[Epoch 189/300] Train Loss=0.999061, Train R^2=-0.0000\n",
      "[Epoch 190/300] Train Loss=1.000609, Train R^2=-0.0000\n",
      "[Epoch 191/300] Train Loss=0.999478, Train R^2=-0.0000\n",
      "[Epoch 192/300] Train Loss=0.999720, Train R^2=-0.0000\n",
      "[Epoch 193/300] Train Loss=1.000108, Train R^2=-0.0000\n",
      "[Epoch 194/300] Train Loss=1.000733, Train R^2=-0.0000\n",
      "[Epoch 195/300] Train Loss=0.999454, Train R^2=-0.0000\n",
      "[Epoch 196/300] Train Loss=0.999690, Train R^2=-0.0000\n",
      "[Epoch 197/300] Train Loss=0.999507, Train R^2=-0.0000\n",
      "[Epoch 198/300] Train Loss=0.999647, Train R^2=-0.0000\n",
      "[Epoch 199/300] Train Loss=0.999542, Train R^2=-0.0000\n",
      "[Epoch 200/300] Train Loss=1.000511, Train R^2=-0.0000\n",
      "[Epoch 201/300] Train Loss=0.999702, Train R^2=-0.0000\n",
      "[Epoch 202/300] Train Loss=0.999462, Train R^2=-0.0000\n",
      "[Epoch 203/300] Train Loss=0.999576, Train R^2=-0.0000\n",
      "[Epoch 204/300] Train Loss=0.999445, Train R^2=-0.0000\n",
      "[Epoch 205/300] Train Loss=1.000103, Train R^2=-0.0000\n",
      "[Epoch 206/300] Train Loss=1.000522, Train R^2=-0.0000\n",
      "[Epoch 207/300] Train Loss=0.999661, Train R^2=-0.0000\n",
      "[Epoch 208/300] Train Loss=0.999875, Train R^2=-0.0000\n",
      "[Epoch 209/300] Train Loss=0.998923, Train R^2=-0.0000\n",
      "[Epoch 210/300] Train Loss=0.999997, Train R^2=-0.0000\n",
      "[Epoch 211/300] Train Loss=0.999713, Train R^2=-0.0000\n",
      "[Epoch 212/300] Train Loss=0.999873, Train R^2=-0.0000\n",
      "[Epoch 213/300] Train Loss=0.999800, Train R^2=-0.0000\n",
      "[Epoch 214/300] Train Loss=1.000583, Train R^2=-0.0000\n",
      "[Epoch 215/300] Train Loss=1.000273, Train R^2=-0.0000\n",
      "[Epoch 216/300] Train Loss=0.999401, Train R^2=-0.0000\n",
      "[Epoch 217/300] Train Loss=0.999412, Train R^2=-0.0000\n",
      "[Epoch 218/300] Train Loss=1.000204, Train R^2=-0.0000\n",
      "[Epoch 219/300] Train Loss=0.999055, Train R^2=-0.0000\n",
      "[Epoch 220/300] Train Loss=1.000591, Train R^2=-0.0000\n",
      "[Epoch 221/300] Train Loss=0.999451, Train R^2=-0.0000\n",
      "[Epoch 222/300] Train Loss=1.001371, Train R^2=-0.0000\n",
      "[Epoch 223/300] Train Loss=0.999379, Train R^2=-0.0000\n",
      "[Epoch 224/300] Train Loss=0.999491, Train R^2=-0.0000\n",
      "[Epoch 225/300] Train Loss=0.999895, Train R^2=-0.0000\n",
      "[Epoch 226/300] Train Loss=0.999492, Train R^2=-0.0000\n",
      "[Epoch 227/300] Train Loss=1.000769, Train R^2=-0.0000\n",
      "[Epoch 228/300] Train Loss=1.000394, Train R^2=-0.0000\n",
      "[Epoch 229/300] Train Loss=0.999989, Train R^2=-0.0000\n",
      "[Epoch 230/300] Train Loss=0.999270, Train R^2=-0.0000\n",
      "[Epoch 231/300] Train Loss=1.000099, Train R^2=-0.0000\n",
      "[Epoch 232/300] Train Loss=1.000848, Train R^2=-0.0000\n",
      "[Epoch 233/300] Train Loss=0.999927, Train R^2=-0.0000\n",
      "[Epoch 234/300] Train Loss=0.998847, Train R^2=-0.0000\n",
      "[Epoch 235/300] Train Loss=0.998803, Train R^2=-0.0000\n",
      "[Epoch 236/300] Train Loss=0.999359, Train R^2=-0.0000\n",
      "[Epoch 237/300] Train Loss=1.000031, Train R^2=-0.0000\n",
      "[Epoch 238/300] Train Loss=0.999953, Train R^2=-0.0000\n",
      "[Epoch 239/300] Train Loss=0.999895, Train R^2=-0.0000\n",
      "[Epoch 240/300] Train Loss=0.999820, Train R^2=-0.0000\n",
      "[Epoch 241/300] Train Loss=0.999645, Train R^2=-0.0000\n",
      "[Epoch 242/300] Train Loss=0.999669, Train R^2=-0.0000\n",
      "[Epoch 243/300] Train Loss=1.000369, Train R^2=-0.0000\n",
      "[Epoch 244/300] Train Loss=1.000258, Train R^2=-0.0000\n",
      "[Epoch 245/300] Train Loss=0.999371, Train R^2=-0.0000\n",
      "[Epoch 246/300] Train Loss=0.998941, Train R^2=-0.0000\n",
      "[Epoch 247/300] Train Loss=0.999935, Train R^2=-0.0000\n",
      "[Epoch 248/300] Train Loss=0.998896, Train R^2=-0.0000\n",
      "[Epoch 249/300] Train Loss=1.000107, Train R^2=-0.0000\n",
      "[Epoch 250/300] Train Loss=0.999006, Train R^2=-0.0000\n",
      "[Epoch 251/300] Train Loss=1.000047, Train R^2=-0.0000\n",
      "[Epoch 252/300] Train Loss=0.999602, Train R^2=-0.0000\n",
      "[Epoch 253/300] Train Loss=0.999268, Train R^2=-0.0000\n",
      "[Epoch 254/300] Train Loss=0.998492, Train R^2=-0.0000\n",
      "[Epoch 255/300] Train Loss=0.999867, Train R^2=-0.0000\n",
      "[Epoch 256/300] Train Loss=1.000320, Train R^2=-0.0000\n",
      "[Epoch 257/300] Train Loss=1.000059, Train R^2=-0.0000\n",
      "[Epoch 258/300] Train Loss=1.000544, Train R^2=-0.0000\n",
      "[Epoch 259/300] Train Loss=0.999388, Train R^2=-0.0000\n",
      "[Epoch 260/300] Train Loss=0.999486, Train R^2=-0.0000\n",
      "[Epoch 261/300] Train Loss=0.999943, Train R^2=-0.0000\n",
      "[Epoch 262/300] Train Loss=0.998548, Train R^2=-0.0000\n",
      "[Epoch 263/300] Train Loss=1.000726, Train R^2=-0.0000\n",
      "[Epoch 264/300] Train Loss=0.999354, Train R^2=-0.0000\n",
      "[Epoch 265/300] Train Loss=0.999792, Train R^2=-0.0000\n",
      "[Epoch 266/300] Train Loss=0.999784, Train R^2=-0.0000\n",
      "[Epoch 267/300] Train Loss=0.999463, Train R^2=-0.0000\n",
      "[Epoch 268/300] Train Loss=1.000560, Train R^2=-0.0000\n",
      "[Epoch 269/300] Train Loss=0.999517, Train R^2=-0.0000\n",
      "[Epoch 270/300] Train Loss=1.000184, Train R^2=-0.0000\n",
      "[Epoch 271/300] Train Loss=0.999323, Train R^2=-0.0000\n",
      "[Epoch 272/300] Train Loss=0.999915, Train R^2=-0.0000\n",
      "[Epoch 273/300] Train Loss=1.001002, Train R^2=-0.0000\n",
      "[Epoch 274/300] Train Loss=0.999653, Train R^2=-0.0000\n",
      "[Epoch 275/300] Train Loss=0.999414, Train R^2=-0.0000\n",
      "[Epoch 276/300] Train Loss=0.999575, Train R^2=-0.0000\n",
      "[Epoch 277/300] Train Loss=0.999729, Train R^2=-0.0000\n",
      "[Epoch 278/300] Train Loss=1.000248, Train R^2=-0.0000\n",
      "[Epoch 279/300] Train Loss=0.999676, Train R^2=-0.0000\n",
      "[Epoch 280/300] Train Loss=1.000121, Train R^2=-0.0000\n",
      "[Epoch 281/300] Train Loss=0.999948, Train R^2=-0.0000\n",
      "[Epoch 282/300] Train Loss=1.000946, Train R^2=-0.0000\n",
      "[Epoch 283/300] Train Loss=1.000888, Train R^2=-0.0000\n",
      "[Epoch 284/300] Train Loss=1.000495, Train R^2=-0.0000\n",
      "[Epoch 285/300] Train Loss=0.999631, Train R^2=-0.0000\n",
      "[Epoch 286/300] Train Loss=0.999311, Train R^2=-0.0000\n",
      "[Epoch 287/300] Train Loss=0.999837, Train R^2=-0.0000\n",
      "[Epoch 288/300] Train Loss=1.000080, Train R^2=-0.0000\n",
      "[Epoch 289/300] Train Loss=1.000014, Train R^2=-0.0000\n",
      "[Epoch 290/300] Train Loss=0.998736, Train R^2=-0.0000\n",
      "[Epoch 291/300] Train Loss=1.000071, Train R^2=-0.0000\n",
      "[Epoch 292/300] Train Loss=1.000644, Train R^2=-0.0000\n",
      "[Epoch 293/300] Train Loss=1.000538, Train R^2=-0.0000\n",
      "[Epoch 294/300] Train Loss=1.000172, Train R^2=-0.0000\n",
      "[Epoch 295/300] Train Loss=1.000798, Train R^2=-0.0000\n",
      "[Epoch 296/300] Train Loss=1.000048, Train R^2=-0.0000\n",
      "[Epoch 297/300] Train Loss=1.000693, Train R^2=-0.0000\n",
      "[Epoch 298/300] Train Loss=0.999710, Train R^2=-0.0000\n",
      "[Epoch 299/300] Train Loss=0.999738, Train R^2=-0.0000\n",
      "[Epoch 300/300] Train Loss=1.000068, Train R^2=-0.0000\n",
      "Shape of train_preds_unnorm: (5000, 229)\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "###############################################################################\n",
    "# 1) LOAD DATA\n",
    "###############################################################################\n",
    "file_path = \"L23_neuron_20210228_Y54_Z320_test.mat\"  # Update with the correct path\n",
    "mat_data = scipy.io.loadmat(file_path)\n",
    "\n",
    "# Convert MATLAB arrays to NumPy arrays\n",
    "eigenface_evoked = np.array(mat_data[\"Eigenface_0_trials_evoked\"])  # shape (500, 1000, 4)\n",
    "eigenface_isi    = np.array(mat_data[\"Eigenface_0_trials_isi\"])     # shape (500, 1000)\n",
    "dff_evoked       = np.array(mat_data[\"dFF0_trials_evoked\"])         # shape (229, 1000, 4)\n",
    "dff_isi          = np.array(mat_data[\"dFF0_trials_isi\"])            # shape (229, 1000)\n",
    "\n",
    "###############################################################################\n",
    "# 2) DATASET (PER-NEURON NORMALIZATION)\n",
    "###############################################################################\n",
    "class NeuralDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each sample:\n",
    "      - x: Concatenation of face(500) + stim(4) => (504,) input dims\n",
    "      - y: (229,) neural targets\n",
    "    \n",
    "    We'll do PER-NEURON normalization on the targets (y).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eigenface_evoked, dff_evoked,\n",
    "                 eigenface_isi, dff_isi,\n",
    "                 apply_norm=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.samples_x = []\n",
    "        self.samples_y = []\n",
    "\n",
    "        n_stim = 4\n",
    "        # ---------- Evoked -------------\n",
    "        for c in range(n_stim):\n",
    "            face_data_cond = eigenface_evoked[:, :, c]   # shape (500, 1000)\n",
    "            neural_data_cond = dff_evoked[:, :, c]       # shape (229, 1000)\n",
    "            stim_one_hot = np.zeros((n_stim,), dtype=np.float32)\n",
    "            stim_one_hot[c] = 1.0\n",
    "\n",
    "            for col in range(face_data_cond.shape[1]):  # 1000\n",
    "                face_col = face_data_cond[:, col].astype(np.float32)     # (500,)\n",
    "                neural_col = neural_data_cond[:, col].astype(np.float32) # (229,)\n",
    "                face_stim = np.concatenate([face_col, stim_one_hot], axis=0)  # (504,)\n",
    "                self.samples_x.append(face_stim)\n",
    "                self.samples_y.append(neural_col)\n",
    "\n",
    "        # ---------- ISI (no stim) -------------\n",
    "        face_isi  = eigenface_isi  # (500, 1000)\n",
    "        neural_isi = dff_isi       # (229, 1000)\n",
    "        zero_stim = np.zeros((n_stim,), dtype=np.float32)\n",
    "\n",
    "        for col in range(face_isi.shape[1]):  # 1000\n",
    "            face_col = face_isi[:, col].astype(np.float32)\n",
    "            neural_col = neural_isi[:, col].astype(np.float32)\n",
    "            face_stim = np.concatenate([face_col, zero_stim], axis=0)\n",
    "            self.samples_x.append(face_stim)\n",
    "            self.samples_y.append(neural_col)\n",
    "\n",
    "        # Convert to Tensors\n",
    "        self.samples_x = torch.tensor(np.array(self.samples_x))  # shape (N, 504)\n",
    "        self.samples_y = torch.tensor(np.array(self.samples_y))  # shape (N, 229)\n",
    "\n",
    "        # If we want to apply per-neuron normalization:\n",
    "        if apply_norm:\n",
    "            # Compute mean/std for each neuron\n",
    "            self.means = self.samples_y.mean(dim=0)    # (229,)\n",
    "            self.stds  = self.samples_y.std(dim=0)     # (229,)\n",
    "            # Avoid division by zero\n",
    "            self.stds  = torch.where(self.stds < 1e-9, torch.ones_like(self.stds), self.stds)\n",
    "\n",
    "            # Normalize each neuron individually\n",
    "            self.samples_y = (self.samples_y - self.means) / self.stds\n",
    "        else:\n",
    "            # No normalization\n",
    "            self.means = torch.zeros((229,), dtype=torch.float32)\n",
    "            self.stds  = torch.ones((229,), dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.samples_x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples_x[idx], self.samples_y[idx]\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 3) SINGLE-BRANCH BIG MLP\n",
    "###############################################################################\n",
    "class BigMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A large MLP to encourage overfitting on the training data.\n",
    "    Input: 504-dim (500 face + 4 stim)\n",
    "    Output: 229-dim (neural response, normalized)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim=504, hidden_dims=[2048, 1024, 512], output_dim=229):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hdim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hdim))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_dim = hdim\n",
    "\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 4) HELPER FUNCTIONS\n",
    "###############################################################################\n",
    "def invert_normalization(y_normed, means, stds):\n",
    "    \"\"\"\n",
    "    Invert the normalization done on the neural outputs.\n",
    "    y_normed: (batch, 229) normalized\n",
    "    means, stds: (229,)\n",
    "    \"\"\"\n",
    "    return y_normed * stds + means\n",
    "\n",
    "def compute_r2(all_preds, all_true):\n",
    "    \"\"\"\n",
    "    Compute R^2 across all samples and all neurons combined (flattened)\n",
    "    OR you can do it per neuron if desired.\n",
    "    \"\"\"\n",
    "    # Flatten out everything: shape (N*229,)\n",
    "    y_true = all_true.view(-1).cpu().numpy()\n",
    "    y_pred = all_preds.view(-1).cpu().numpy()\n",
    "    \n",
    "    ss_res = np.sum((y_true - y_pred)**2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true))**2)\n",
    "    if ss_tot < 1e-12:\n",
    "        return 0.0\n",
    "    return 1.0 - ss_res / ss_tot\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 5) TRAINING LOOP (FOCUSED ON TRAINING ONLY)\n",
    "###############################################################################\n",
    "def train_for_overfitting(eigenface_evoked, dff_evoked,\n",
    "                          eigenface_isi, dff_isi,\n",
    "                          epochs=300,\n",
    "                          batch_size=64,\n",
    "                          lr=1e-3,\n",
    "                          device='cuda'):\n",
    "\n",
    "    # Create the full dataset. We do NOT split into val/test\n",
    "    dataset = NeuralDataset(eigenface_evoked, dff_evoked,\n",
    "                            eigenface_isi, dff_isi,\n",
    "                            apply_norm=True)\n",
    "    means, stds = dataset.means, dataset.stds\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "    # Create the model\n",
    "    model = BigMLP(input_dim=504, hidden_dims=[2048, 1024, 512], output_dim=229).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)  # normalized\n",
    "            pred = model(batch_x)\n",
    "            loss = criterion(pred, batch_y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Compute R^2 on the entire training set (optional, but nice to see progress)\n",
    "        # We'll do it in a quick pass:\n",
    "        model.eval()\n",
    "        all_preds, all_true = [], []\n",
    "        with torch.no_grad():\n",
    "            for bx, by in train_loader:\n",
    "                bx = bx.to(device)\n",
    "                by = by.to(device)\n",
    "                py = model(bx)\n",
    "                all_preds.append(py)\n",
    "                all_true.append(by)\n",
    "        \n",
    "        # Concatenate\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_true  = torch.cat(all_true, dim=0)\n",
    "        r2_train  = compute_r2(all_preds, all_true)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}/{epochs}] Train Loss={train_loss:.6f}, Train R^2={r2_train:.4f}\")\n",
    "\n",
    "    # Final model + dataset stats\n",
    "    return model, (means, stds), dataset\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 6) INFERENCE EXAMPLE\n",
    "###############################################################################\n",
    "def predict_entire_dataset(model, dataset, means, stds, device='cuda', batch_size=64):\n",
    "    \"\"\"\n",
    "    Return unnormalized predictions for the entire dataset.\n",
    "    \"\"\"\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for bx, _ in loader:\n",
    "            bx = bx.to(device)\n",
    "            py_norm = model(bx)  # normalized\n",
    "            # invert normalization\n",
    "            py_real = invert_normalization(py_norm, means.to(device), stds.to(device))\n",
    "            all_preds.append(py_real.cpu())\n",
    "    return torch.cat(all_preds, dim=0).numpy()\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 7) MAIN\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Train only on the entire dataset (no validation).\n",
    "    model, (means, stds), train_dataset = train_for_overfitting(\n",
    "        eigenface_evoked, dff_evoked,\n",
    "        eigenface_isi, dff_isi,\n",
    "        epochs=300,    # Increase if you still want to push training R^2 even higher\n",
    "        batch_size=64,\n",
    "        lr=1e-3,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # After training, you can get final predictions on the training set\n",
    "    train_preds_unnorm = predict_entire_dataset(model, train_dataset, means, stds, device=device)\n",
    "    print(\"Shape of train_preds_unnorm:\", train_preds_unnorm.shape)  # (N, 229)\n",
    "    \n",
    "    # ...and do whatever analysis/plots you like on train_preds_unnorm.\n",
    "    # Since we don't care about test/validation, we won't show that here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5888abb0-9eb6-45f6-a71c-1cc3aad17d68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115a5ccd-93a1-4e0a-bf23-0e8ae1020256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9915e7-8e55-4001-8ad9-d8853b22afad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
