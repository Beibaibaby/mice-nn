{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0f8969d-bfb2-4cf3-9dd0-b37ad4c37b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "\n",
    "# Load the .mat file\n",
    "file_path = \"L23_neuron_20210228_Y54_Z320_test.mat\"  # Update with the correct path\n",
    "mat_data = scipy.io.loadmat(file_path)\n",
    "\n",
    "# Convert MATLAB arrays to NumPy arrays\n",
    "data_dict = {\n",
    "    \"Eigenface_0_trials_evoked\": np.array(mat_data[\"Eigenface_0_trials_evoked\"]),\n",
    "    \"Eigenface_0_trials_isi\": np.array(mat_data[\"Eigenface_0_trials_isi\"]),\n",
    "    \"dFF0_trials_evoked\": np.array(mat_data[\"dFF0_trials_evoked\"]),\n",
    "    \"dFF0_trials_isi\": np.array(mat_data[\"dFF0_trials_isi\"]),\n",
    "}\n",
    "eigenface_evoked = []\n",
    "dff_evoked =[]\n",
    "eigenface_isi =[]\n",
    "dff_isi =[]\n",
    "\n",
    "# Direct assignment from the data_dict\n",
    "eigenface_evoked = data_dict[\"Eigenface_0_trials_evoked\"]\n",
    "eigenface_isi = data_dict[\"Eigenface_0_trials_isi\"]\n",
    "dff_evoked = data_dict[\"dFF0_trials_evoked\"]\n",
    "dff_isi = data_dict[\"dFF0_trials_isi\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5888abb0-9eb6-45f6-a71c-1cc3aad17d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NeuralDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom Dataset to handle both evoked (with stimulus) and ISI (no-stimulus) data.\n",
    "    We create samples of shape (seq_len, input_dim) for face motion,\n",
    "    along with a stimulus indicator and the corresponding neural targets.\n",
    "    \n",
    "    This code will:\n",
    "      - Separate evoked data by condition.\n",
    "      - Combine with ISI data as 'no-stimulus' condition.\n",
    "      - Possibly chunk into smaller sequences for the Transformer.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 eigenface_evoked,   # shape (500, 1000, 4)\n",
    "                 dff_evoked,         # shape (229, 1000, 4)\n",
    "                 eigenface_isi,      # shape (500, 1000)\n",
    "                 dff_isi,            # shape (229, 1000)\n",
    "                 seq_len=100,\n",
    "                 device='cpu'):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.device = device\n",
    "\n",
    "        # Evoked data: separate each condition\n",
    "        # We will end up with 4 separate sequences for face & neural\n",
    "        # each shape: (500, 1000)\n",
    "        self.face_evoked_list = []\n",
    "        self.neural_evoked_list = []\n",
    "        self.stimulus_id_list = []  # store integer or one-hot for each condition\n",
    "\n",
    "        for cond_idx in range(4):\n",
    "            face_data_cond = eigenface_evoked[:, :, cond_idx]  # shape (500, 1000)\n",
    "            neural_data_cond = dff_evoked[:, :, cond_idx]       # shape (229, 1000)\n",
    "            \n",
    "            # We'll store them as Tensors\n",
    "            face_data_cond = torch.tensor(face_data_cond, dtype=torch.float32)\n",
    "            neural_data_cond = torch.tensor(neural_data_cond, dtype=torch.float32)\n",
    "            \n",
    "            self.face_evoked_list.append(face_data_cond)\n",
    "            self.neural_evoked_list.append(neural_data_cond)\n",
    "            \n",
    "            # Stimulus one-hot or ID\n",
    "            # e.g. cond_idx ∈ {0,1,2,3} => one-hot of length 4 \n",
    "            stim_one_hot = torch.zeros(4)\n",
    "            stim_one_hot[cond_idx] = 1.0\n",
    "            self.stimulus_id_list.append(stim_one_hot)\n",
    "        \n",
    "        # ISI data (no stimulus)\n",
    "        # shape (500, 1000) for face, (229, 1000) for neural\n",
    "        self.face_isi = torch.tensor(eigenface_isi, dtype=torch.float32)\n",
    "        self.neural_isi = torch.tensor(dff_isi, dtype=torch.float32)\n",
    "\n",
    "        # Build a list of all sequences (face, neural, stim_vector)\n",
    "        # for evoked:\n",
    "        self.samples = []\n",
    "        for i in range(4):\n",
    "            face_seq = self.face_evoked_list[i]     # (500, 1000)\n",
    "            neural_seq = self.neural_evoked_list[i] # (229, 1000)\n",
    "            stim_vec = self.stimulus_id_list[i]     # (4,)\n",
    "            \n",
    "            # Chunk the 1000 frames into segments of seq_len\n",
    "            # e.g. for start in [0, seq_len, 2*seq_len, ...]\n",
    "            # Make sure we only keep full segments\n",
    "            n_frames = face_seq.shape[1]  # 1000\n",
    "            for start in range(0, n_frames, seq_len):\n",
    "                end = start + seq_len\n",
    "                if end > n_frames:\n",
    "                    break\n",
    "                face_chunk = face_seq[:, start:end]   # shape (500, seq_len)\n",
    "                neural_chunk = neural_seq[:, start:end]  # shape (229, seq_len)\n",
    "                self.samples.append((face_chunk, stim_vec, neural_chunk, True))  \n",
    "                # last param = True indicates \"stimulus present\"\n",
    "        \n",
    "        # for ISI:\n",
    "        n_frames_isi = self.face_isi.shape[1]  # 1000\n",
    "        for start in range(0, n_frames_isi, seq_len):\n",
    "            end = start + seq_len\n",
    "            if end > n_frames_isi:\n",
    "                break\n",
    "            face_chunk = self.face_isi[:, start:end]   # shape (500, seq_len)\n",
    "            neural_chunk = self.neural_isi[:, start:end]  # shape (229, seq_len)\n",
    "            # Stim is zero vector => no stimulus\n",
    "            stim_vec = torch.zeros(4)\n",
    "            self.samples.append((face_chunk, stim_vec, neural_chunk, False))\n",
    "        \n",
    "        # We have a big list of (face_chunk, stim_one_hot, neural_chunk, has_stim)\n",
    "        # We'll shuffle them in the DataLoader.  \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        face_chunk, stim_vec, neural_chunk, has_stim = self.samples[idx]\n",
    "\n",
    "        # shape transformations if needed:\n",
    "        # Now face_chunk: (500, seq_len). We want it as (seq_len, 500) for the Transformer\n",
    "        face_chunk = face_chunk.permute(1, 0)  # shape (seq_len, 500)\n",
    "        neural_chunk = neural_chunk.permute(1, 0)  # shape (seq_len, 229)\n",
    "        \n",
    "        return (face_chunk, stim_vec, neural_chunk, has_stim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "115a5ccd-93a1-4e0a-bf23-0e8ae1020256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultimodalTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 face_dim=500,       # input dimension of face PCA features\n",
    "                 stim_dim=4,         # one-hot size for the 4 stimuli\n",
    "                 d_model=128,        # transformer embedding dimension\n",
    "                 nhead=4,            # number of attention heads\n",
    "                 num_layers=2,       # number of transformer encoder layers\n",
    "                 hidden_mlp=256,     # feedforward dimension in Transformer\n",
    "                 output_dim=229,     # number of neurons to predict\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.face_dim = face_dim\n",
    "        self.stim_dim = stim_dim\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # 1) Linear projection for face input => d_model\n",
    "        self.face_embedding = nn.Linear(face_dim, d_model)\n",
    "        \n",
    "        # 2) Embedding for stimulus => project 4-d one-hot to d_model\n",
    "        self.stim_embedding = nn.Linear(stim_dim, d_model)\n",
    "        \n",
    "        # 3) Positional encoding (basic)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=dropout)\n",
    "        \n",
    "        # 4) Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model,\n",
    "                                                   nhead=nhead,\n",
    "                                                   dim_feedforward=hidden_mlp,\n",
    "                                                   dropout=dropout,\n",
    "                                                   activation='relu')\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer,\n",
    "                                                         num_layers=num_layers)\n",
    "        \n",
    "        # 5) Output head: map from d_model -> output_dim (229)\n",
    "        self.output_layer = nn.Linear(d_model, output_dim)\n",
    "        \n",
    "    def forward(self, face_seq, stim_vec, has_stim=None):\n",
    "        \"\"\"\n",
    "        face_seq: (batch_size, seq_len, face_dim)\n",
    "        stim_vec: (batch_size, stim_dim) - one-hot\n",
    "        has_stim: (batch_size,) boolean or None\n",
    "        \"\"\"\n",
    "        # Project face input to d_model\n",
    "        B, T, _ = face_seq.shape\n",
    "        \n",
    "        face_embed = self.face_embedding(face_seq)  # (B, T, d_model)\n",
    "        \n",
    "        # Expand stimulus embedding across time:\n",
    "        #  stim_vec => (B, 1, d_model), then tile to (B, T, d_model)\n",
    "        stim_embed = self.stim_embedding(stim_vec)  # (B, d_model)\n",
    "        stim_embed = stim_embed.unsqueeze(1).expand(-1, T, -1)  # (B, T, d_model)\n",
    "        \n",
    "        # If we have a \"no stimulus\" case, we can zero out or learn a separate embedding\n",
    "        # For now, just keep stim_embed for both. Another approach:\n",
    "        if has_stim is not None:\n",
    "            # has_stim is a bool vector of length B\n",
    "            # we can zero out stim_embed if has_stim=False\n",
    "            mask = has_stim.view(-1, 1, 1).float()  # shape (B,1,1)\n",
    "            stim_embed = stim_embed * mask  # zero if no stimulus\n",
    "        # Alternatively, use something like a separate \"no stim\" embedding. \n",
    "        # This is flexible.\n",
    "        \n",
    "        # Combine face + stimulus by adding or concatenating.\n",
    "        # A typical approach in transformers is to just add them (like we do with position encoding).\n",
    "        # Or we can also concatenate them along dimension=2 and use a linear to get back to d_model.\n",
    "        # Let's add them:\n",
    "        x = face_embed + stim_embed  # shape (B, T, d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.positional_encoding(x)  # (B, T, d_model)\n",
    "        \n",
    "        # Transformer expects shape (T, B, d_model), so transpose:\n",
    "        x = x.permute(1, 0, 2)  # (T, B, d_model)\n",
    "        \n",
    "        # Pass through the transformer encoder\n",
    "        encoded = self.transformer_encoder(x)  # (T, B, d_model)\n",
    "        \n",
    "        # Convert back to (B, T, d_model)\n",
    "        encoded = encoded.permute(1, 0, 2)\n",
    "        \n",
    "        # Predict neural activity at each time step\n",
    "        out = self.output_layer(encoded)  # (B, T, 229)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard positional encoding for transformers.\n",
    "    This version inserts sinusoidal positional embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=10000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                             (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # shape (1, max_len, d_model)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        # Add positional encoding\n",
    "        x = x + self.pe[:, :seq_len, :]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "077e65ca-da2d-49ac-a1a2-fa2238b90c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_model(eigenface_evoked, dff_evoked,\n",
    "                eigenface_isi, dff_isi,\n",
    "                epochs=10,\n",
    "                batch_size=8,\n",
    "                seq_len=100,\n",
    "                lr=1e-3,\n",
    "                device='cuda'):\n",
    "    \n",
    "    # 1) Build dataset & dataloader\n",
    "    dataset = NeuralDataset(eigenface_evoked, dff_evoked,\n",
    "                            eigenface_isi, dff_isi,\n",
    "                            seq_len=seq_len,\n",
    "                            device=device)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    # 2) Instantiate the model\n",
    "    model = MultimodalTransformer(\n",
    "        face_dim=500,\n",
    "        stim_dim=4,\n",
    "        d_model=128,\n",
    "        nhead=4,\n",
    "        num_layers=2,\n",
    "        hidden_mlp=256,\n",
    "        output_dim=229,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "    \n",
    "    # 3) Loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # 4) Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            face_chunk, stim_vec, neural_chunk, has_stim = batch\n",
    "            # face_chunk : (B, seq_len, 500)\n",
    "            # stim_vec   : (B, 4)\n",
    "            # neural_chunk : (B, seq_len, 229)\n",
    "            # has_stim   : (B,) bool\n",
    "            face_chunk = face_chunk.to(device)\n",
    "            stim_vec   = stim_vec.to(device)\n",
    "            neural_chunk = neural_chunk.to(device)\n",
    "            has_stim   = has_stim.to(device)\n",
    "            \n",
    "            # ----- Modality Dropout (optional) -----\n",
    "            # randomly zero out face or stim with some probability\n",
    "            drop_prob = 0.1  # 10% chance to drop a modality\n",
    "            if torch.rand(1).item() < drop_prob:\n",
    "                # drop face\n",
    "                face_chunk = torch.zeros_like(face_chunk)\n",
    "            if torch.rand(1).item() < drop_prob:\n",
    "                # drop stimulus\n",
    "                stim_vec = torch.zeros_like(stim_vec)\n",
    "                has_stim = torch.zeros_like(has_stim)  # effectively no stim\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward\n",
    "            pred = model(face_chunk, stim_vec, has_stim=has_stim)\n",
    "            # pred shape: (B, seq_len, 229)\n",
    "            \n",
    "            # compute loss\n",
    "            loss = criterion(pred, neural_chunk)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b4bbfdc-13bc-4e31-82b0-b4dd3917d1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dracoxu/miniconda3/envs/research/lib/python3.12/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.2409\n",
      "Epoch [2/50], Loss: 0.0530\n",
      "Epoch [3/50], Loss: 0.0345\n",
      "Epoch [4/50], Loss: 0.0263\n",
      "Epoch [5/50], Loss: 0.0244\n",
      "Epoch [6/50], Loss: 0.0226\n",
      "Epoch [7/50], Loss: 0.0210\n",
      "Epoch [8/50], Loss: 0.0193\n",
      "Epoch [9/50], Loss: 0.0186\n",
      "Epoch [10/50], Loss: 0.0181\n",
      "Epoch [11/50], Loss: 0.0176\n",
      "Epoch [12/50], Loss: 0.0169\n",
      "Epoch [13/50], Loss: 0.0165\n",
      "Epoch [14/50], Loss: 0.0160\n",
      "Epoch [15/50], Loss: 0.0157\n",
      "Epoch [16/50], Loss: 0.0154\n",
      "Epoch [17/50], Loss: 0.0148\n",
      "Epoch [18/50], Loss: 0.0145\n",
      "Epoch [19/50], Loss: 0.0141\n",
      "Epoch [20/50], Loss: 0.0142\n",
      "Epoch [21/50], Loss: 0.0137\n",
      "Epoch [22/50], Loss: 0.0133\n",
      "Epoch [23/50], Loss: 0.0131\n",
      "Epoch [24/50], Loss: 0.0128\n",
      "Epoch [25/50], Loss: 0.0127\n",
      "Epoch [26/50], Loss: 0.0124\n",
      "Epoch [27/50], Loss: 0.0122\n",
      "Epoch [28/50], Loss: 0.0121\n",
      "Epoch [29/50], Loss: 0.0118\n",
      "Epoch [30/50], Loss: 0.0116\n",
      "Epoch [31/50], Loss: 0.0118\n",
      "Epoch [32/50], Loss: 0.0117\n",
      "Epoch [33/50], Loss: 0.0114\n",
      "Epoch [34/50], Loss: 0.0110\n",
      "Epoch [35/50], Loss: 0.0109\n",
      "Epoch [36/50], Loss: 0.0107\n",
      "Epoch [37/50], Loss: 0.0105\n",
      "Epoch [38/50], Loss: 0.0102\n",
      "Epoch [39/50], Loss: 0.0102\n",
      "Epoch [40/50], Loss: 0.0099\n",
      "Epoch [41/50], Loss: 0.0099\n",
      "Epoch [42/50], Loss: 0.0097\n",
      "Epoch [43/50], Loss: 0.0096\n",
      "Epoch [44/50], Loss: 0.0095\n",
      "Epoch [45/50], Loss: 0.0093\n",
      "Epoch [46/50], Loss: 0.0092\n",
      "Epoch [47/50], Loss: 0.0093\n",
      "Epoch [48/50], Loss: 0.0090\n",
      "Epoch [49/50], Loss: 0.0090\n",
      "Epoch [50/50], Loss: 0.0088\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = train_model(\n",
    "     eigenface_evoked, dff_evoked,\n",
    "     eigenface_isi, dff_isi,\n",
    "     epochs=50, batch_size=8, seq_len=100, lr=1e-3, device=device\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af704c0b-39ea-4950-bdcd-279c88205428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f276047e-de32-49e3-a1eb-b0d57f870dff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f8ae83b-ada4-4bb7-b15f-373cb1bb3a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9428372-8452-4ad8-96a2-890665803787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "487f2b68-1bb8-48b4-ad3c-70a3f9a0378e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d21c9571-dbc4-4aec-b83b-f5158db12820",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_eval_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 1) Suppose you've already trained your model:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# model = train_model(...)  # your custom function\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 2) Suppose you have a DataLoader to evaluate on:\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m eval_loader \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_eval_loader\u001b[49m(\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 3) Get predictions:\u001b[39;00m\n\u001b[1;32m      9\u001b[0m all_preds, all_actual \u001b[38;5;241m=\u001b[39m compute_predictions(model, eval_loader, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_eval_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# 1) Suppose you've already trained your model:\n",
    "# model = train_model(...)  # your custom function\n",
    "# \n",
    "# 2) Suppose you have a DataLoader to evaluate on:\n",
    "eval_loader = create_eval_loader(...)\n",
    "#\n",
    "# 3) Get predictions:\n",
    "\n",
    "all_preds, all_actual = compute_predictions(model, eval_loader, device=device)\n",
    "\n",
    "# 4) Plot a time-series trace for a single neuron:\n",
    "\n",
    "plot_single_neuron_trace(all_preds, all_actual, sample_idx=0, neuron_id=0)\n",
    "\n",
    "# 5) Compute and plot R² for each neuron:\n",
    "\n",
    "r2_vals = compute_r2_per_neuron(all_preds, all_actual)\n",
    "plot_r2_distribution(r2_vals)\n",
    "\n",
    "# 6) Scatter plot predicted vs actual for neuron 0, flattening across all batches:\n",
    "\n",
    "plot_scatter_pred_vs_actual(all_preds, all_actual, neuron_id=0, sample_idx=None)\n",
    "\n",
    "# 7) Plot PCA trajectories for one sample:\n",
    "\n",
    "plot_pca_trajectory(all_preds, all_actual, sample_idx=0, n_components=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af83015d-95de-4b10-914e-171b3120a3cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
